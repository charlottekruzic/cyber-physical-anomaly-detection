{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enregistrement de tables de variances expliquées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook permet d'enregistrer les tables de variances expliquées pour streamlite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickleshare import PickleShareDB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous chargeons les données depuis le fichier des données préparées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PickleShareDB('../prep_data/kity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données nettoyées\n",
    "df_net_1 = db['net_attack_1_clean']\n",
    "df_net_2 = db['net_attack_2_clean']\n",
    "df_net_3 = db['net_attack_3_clean']\n",
    "df_net_4 = db['net_attack_4_clean']\n",
    "df_net_norm = db['net_norm_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dataset avec toutes les données\n",
    "#df = [db['net_attack_1_clean'], db['net_attack_2_clean'], db['net_attack_3_clean'], db['net_attack_4_clean'], db['net_norm_clean']]\n",
    "#df = pd.concat(df, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation(df):\n",
    "    df = df.drop(columns=['Time'])\n",
    "    scaler = StandardScaler()\n",
    "    df[['n_pkt_src', 'n_pkt_dst', 'size']] = scaler.fit_transform(df[['n_pkt_src', 'n_pkt_dst', 'size']])\n",
    "    labels = df[['label', 'label_n']]\n",
    "    df = df.drop(columns=['label', 'label_n'])\n",
    "    df = pd.get_dummies(df, columns=['flags',  'proto', 'modbus_fn', 'modbus_response', 'dport', 'sport'], prefix=['flags', 'proto', 'modbus_fn', 'modbus_response', 'dport', 'sport'])\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    df[['ip_s', 'ip_d', 'mac_s', 'mac_d']] = ordinal_encoder.fit_transform(df[['ip_s', 'ip_d', 'mac_s', 'mac_d']])\n",
    "    scaler = StandardScaler()\n",
    "    df[['ip_s', 'ip_d', 'mac_s', 'mac_d']] = scaler.fit_transform(df[['ip_s', 'ip_d', 'mac_s', 'mac_d']])\n",
    "    df.drop(columns=['modbus_fn_inconnue', 'flags_inconnue',  'modbus_response_inconnue'], inplace=True) #'sport_inconnu', 'dport_inconnu',\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'bool':\n",
    "            df[col] = df[col].astype(float)\n",
    "    return df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_net_1 prêt\n",
      "df_net_2 prêt\n",
      "df_net_3 prêt\n",
      "df_net_4 prêt\n"
     ]
    }
   ],
   "source": [
    "df_net_1, labels_1 =  preparation(df_net_1)\n",
    "print(\"df_net_1 prêt\")\n",
    "df_net_2, labels_2 =  preparation(df_net_2)\n",
    "print(\"df_net_2 prêt\")\n",
    "df_net_3, labels_3 =  preparation(df_net_3)\n",
    "print(\"df_net_3 prêt\")\n",
    "df_net_4, labels_4 =  preparation(df_net_4)\n",
    "print(\"df_net_4 prêt\")\n",
    "df_net_norm, labels_norm =  preparation(df_net_norm)\n",
    "#print(\"df_net_norm prêt\")\n",
    "#df, labels = preparation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduction de dimensions : PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance expliquée par les dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(df, name):\n",
    "    pca = PCA()\n",
    "    df_pca = pca.fit(df)\n",
    "    variance_expliquee = (pca.explained_variance_ratio_ * 100).round(2)\n",
    "    #variance_expliquee = variance_expliquee.round(2)\n",
    "\n",
    "    variance_table = pd.DataFrame({\n",
    "        \"Composante\": [f\"PC{i+1}\" for i in range(len(variance_expliquee))],\n",
    "        \"Pourcentage de Variance Expliquée\": variance_expliquee,\n",
    "        \"Total\": variance_expliquee.cumsum()\n",
    "    })\n",
    "\n",
    "    db['pca_variance_table_'+ name] = variance_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca(df, 'all')\n",
    "pca(df_net_1, 'net_1')\n",
    "pca(df_net_2, 'net_2')\n",
    "pca(df_net_3, 'net_3')\n",
    "pca(df_net_4, 'net_4')\n",
    "pca(df_net_norm, 'net_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de dimensions minimum permettant de conserver au moins 90% de l'information :\n",
    "- df_1 : 6 dimensions (PC6  2.65  91.94)\n",
    "- df_2 : 6 dimensions (PC6  2.26  91.29)\n",
    "- df_3 : 4 dimensions (PC4  2.33  91.95)\n",
    "- df_4 : 6 dimensions (PC6  2.62  90.53)\n",
    "- df_norm : pas d'interêt\n",
    "- all : 6 dimensions (PC6  2.22  91.00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
