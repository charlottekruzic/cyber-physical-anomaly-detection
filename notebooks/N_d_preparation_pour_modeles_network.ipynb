{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pickleshare import PickleShareDB\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour garantir la reproductibilité\n",
    "np.random.seed(42)  # Pour numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "Nous chargeons les données depuis le fichier des données PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données nettoyées\n",
    "db = PickleShareDB('../prep_data/kity')\n",
    "\n",
    "sample_df = db['pca_result_with_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "normal            3174002\n",
       "DoS               1290386\n",
       "MITM               538852\n",
       "physical fault     387126\n",
       "anomaly                97\n",
       "scan                   15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le type d'attaque \"scan\" possède très peu d'entrées, ce qui empêche les modèles de s'entraîner correctement. Nous supprimons donc ce type d'attaque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = sample_df[sample_df['label'] == 'scan'].index\n",
    "sample_df = sample_df.drop(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons préparé les données pour les classifications en séparant les caractéristiques et les étiquettes. Les données ont ensuite été divisées en ensembles d'entraînement et de test, avec une stratification pour préserver la distribution des classes. Enfin, un oversampling avec SMOTE a été appliqué sur l'ensemble d'entraînement pour équilibrer les classes, en augmentant artificiellement la représentation de la classe minoritaire (attaques)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction de la taille du dataset\n",
    "Nous allons utiliser un dataset échantillon représentant 10 % du dataset original.  \n",
    "Cela permet de conserver une partie significative des données sans que le dataset soit considéré comme volumineux, ce qui serait coûteux en termes de calcul et de mémoire.  \n",
    "Nous effectuons un échantillonnage tout en respectant la distribution du dataset complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157361/3490013237.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sample_df = sample_df.groupby('label', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "sample_df = sample_df.groupby('label', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=0.1, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "normal            317400\n",
       "DoS               129039\n",
       "MITM               53885\n",
       "physical fault     38713\n",
       "anomaly               10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des étiquettes et suppression des colonnes correspondantes du DataFrame\n",
    "labels = sample_df[['label', 'label_n']]\n",
    "sample_df.drop(columns=['label', 'label_n'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation pour la classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les données d'entrée (X) et les étiquettes (y) pour la classification binaire\n",
    "X = sample_df\n",
    "y_label_n = labels['label_n']\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_label_n, test_size=0.2, random_state=42, stratify=y_label_n\n",
    ")\n",
    "\n",
    "# Équilibrage des classes via SMOTE (oversampling des classes minoritaires)\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: False, Count: 253920\n",
      "Value: True, Count: 253920\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts\n",
    "unique_values, counts = np.unique(y_train_resampled, return_counts=True)\n",
    "\n",
    "# Display the result\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"Value: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrement dans PickleShare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des données de classification binaire dans PickleShareDB\n",
    "db['binary_X_train_resampled'] = X_train_resampled\n",
    "db['binary_y_train_resampled'] = y_train_resampled\n",
    "db['binary_X_test'] = X_test\n",
    "db['binary_y_test'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation pour la classification multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les étiquettes pour la classification multi-classes\n",
    "y_label = labels['label']\n",
    "\n",
    "# Liste ordonnée des classes (pour garantir un encodage cohérent des labels)\n",
    "ordered_classes = ['normal', 'DoS', 'physical fault', 'MITM', 'anomaly']\n",
    "\n",
    "# Encodage des étiquettes avec LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(ordered_classes)\n",
    "label_mapping_network = {label: encoded for label, encoded in zip(le.classes_, range(len(le.classes_)))}\n",
    "y_label_encoded = le.transform(y_label)\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_label_encoded, test_size=0.2, random_state=42, stratify=y_label_encoded\n",
    ")\n",
    "\n",
    "# Équilibrage des classes via SMOTE (oversampling des classes minoritaires)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrement dans PickleShare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des données de classification multi-classes dans PickleShareDB\n",
    "db['multiclass_X_train_resampled'] = X_train_resampled\n",
    "db['multiclass_y_train_resampled'] = y_train_resampled\n",
    "db['multiclass_X_test'] = X_test\n",
    "db['multiclass_y_test'] = y_test\n",
    "db['multiclass_label_mapping'] = label_mapping_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['label_mapping_network'] = label_mapping_network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
