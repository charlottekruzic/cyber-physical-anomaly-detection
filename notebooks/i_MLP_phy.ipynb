{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des données physiques avec MLP\n",
    "\n",
    "Ce notebook implémente la classification par Perceptron Multi-Couches (MLP) sur notre jeu de données physiques pour la détection d'attaques. Nous évaluerons les performances à la fois sur la classification binaire (attaque vs normal) et la classification multiclasse (types d'attaques spécifiques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pickleshare import PickleShareDB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score,\n",
    "    confusion_matrix, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "import tracemalloc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from prep_data\n",
    "data_dir = '../prep_data' \n",
    "db = PickleShareDB(os.path.join(data_dir, 'kity'))\n",
    "\n",
    "# Load raw data\n",
    "df_phy_1 = db['df_phy_1']\n",
    "df_phy_2 = db['df_phy_2']\n",
    "df_phy_3 = db['df_phy_3']\n",
    "df_phy_4 = db['df_phy_4']\n",
    "df_phy_norm = db['df_phy_norm']\n",
    "\n",
    "# Load label mapping\n",
    "label_mapping = db['label_mapping']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "\n",
    "Pour MLP, nous devons préparer soigneusement nos données :\n",
    "\n",
    "- Supprimer la colonne Time car l'information temporelle n'est pas directement utile pour la classification\n",
    "- Garder les mesures des capteurs de réservoir et de débit comme indicateurs physiques clés\n",
    "- Garder les états des vannes et des pompes car ils représentent les actions du système\n",
    "- Mettre à l'échelle les caractéristiques numériques car les MLP sont sensibles à la mise à l'échelle des caractéristiques\n",
    "- Encoder en one-hot les variables catégorielles\n",
    "- Supprimer toutes les colonnes constantes ou redondantes identifiées lors d'analyses précédentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \"\"\"Prepare data with proper handling of missing values\"\"\"\n",
    "    # Remove Time column\n",
    "    df_prepared = df.drop(columns=['Time'])\n",
    "    \n",
    "    # Split features and labels\n",
    "    X = df_prepared.drop(columns=['Label', 'Label_n'])\n",
    "    y_label = df_prepared['Label']\n",
    "    y_label_n = df_prepared['Label_n']\n",
    "    \n",
    "    return X, y_label, y_label_n\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess data with imputation and scaling\"\"\"\n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['category', 'bool']).columns\n",
    "    \n",
    "    # Handle numeric features\n",
    "    if len(numeric_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy='mean')\n",
    "        scaler = StandardScaler()\n",
    "        df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    # Handle categorical features\n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "        df = pd.get_dummies(df, columns=categorical_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"Preparing datasets...\")\n",
    "prepared_data = {}\n",
    "for name, df in {\n",
    "    'phy_1': df_phy_1,\n",
    "    'phy_2': df_phy_2,\n",
    "    'phy_3': df_phy_3,\n",
    "    'phy_4': df_phy_4,\n",
    "    'phy_norm': df_phy_norm\n",
    "}.items():\n",
    "    X, y_label, y_label_n = prepare_data(df)\n",
    "    X_processed = preprocess_data(X)\n",
    "    prepared_data[name] = {\n",
    "        'X': X_processed,\n",
    "        'y_label': y_label,\n",
    "        'y_label_n': y_label_n\n",
    "    }\n",
    "\n",
    "# Combine datasets\n",
    "X_all = pd.concat([data['X'] for data in prepared_data.values()])\n",
    "y_label_all = pd.concat([data['y_label'] for data in prepared_data.values()])\n",
    "y_label_n_all = pd.concat([data['y_label_n'] for data in prepared_data.values()])\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "print(\"X_all shape:\", X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation et fonctions d'évaluation MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_mlp(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate MLP model with performance metrics\"\"\"\n",
    "    print(\"NaN check in train_evaluate_mlp:\")\n",
    "    print(\"X_train NaNs:\", X_train.isna().sum().sum())\n",
    "    print(\"X_test NaNs:\", X_test.isna().sum().sum())\n",
    "    \n",
    "    # Handle any remaining NaN values if needed\n",
    "    if X_train.isna().sum().sum() > 0 or X_test.isna().sum().sum() > 0:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "        X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Measure training time and memory\n",
    "    tracemalloc.start()\n",
    "    start_fit_time = time.time()\n",
    "    \n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    fit_time = time.time() - start_fit_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    fit_memory_usage = peak / (1024 * 1024)  # Convert to MB\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    # Measure prediction time and memory\n",
    "    tracemalloc.start()\n",
    "    start_predict_time = time.time()\n",
    "    \n",
    "    y_pred = mlp.predict(X_test)\n",
    "    \n",
    "    predict_time = time.time() - start_predict_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    predict_memory_usage = peak / (1024 * 1024)  # Convert to MB\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'FN': FN,\n",
    "        'precision': precision_score(y_test, y_pred, average='binary'),\n",
    "        'recall': recall_score(y_test, y_pred, average='binary'),\n",
    "        'tnr': TN / (TN + FP) if (TN + FP) != 0 else 0,\n",
    "        'fpr': FP / (FP + TN) if (FP + TN) != 0 else 0,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='binary'),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "        'mcc': matthews_corrcoef(y_test, y_pred),\n",
    "        'fit_time': fit_time,\n",
    "        'predict_time': predict_time,\n",
    "        'fit_memory_usage': fit_memory_usage,\n",
    "        'predict_memory_usage': predict_memory_usage\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement et évaluation du modèle - Classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for binary classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_label_n_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train and evaluate model\n",
    "binary_results = train_evaluate_mlp(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Add metadata\n",
    "binary_results['data'] = 'PHY'\n",
    "binary_results['model_type'] = 'mlp'\n",
    "binary_results['attack_type'] = 'labeln'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les résultats de la classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=binary_results['confusion_matrix'],\n",
    "    x=['Predicted Negative', 'Predicted Positive'],\n",
    "    y=['Actual Negative', 'Actual Positive'],\n",
    "    text=binary_results['confusion_matrix'],\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 16},\n",
    "    colorscale='Blues'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix - Binary Classification',\n",
    "    height=400,\n",
    "    width=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(list(label_mapping.keys()))\n",
    "y_label_all_encoded = le.transform(y_label_all)\n",
    "\n",
    "# Dictionary to store results for each attack type\n",
    "multiclass_results = {}\n",
    "\n",
    "# Train and evaluate for each attack type\n",
    "for attack_label, encoded_label in label_mapping.items():\n",
    "    if attack_label != 'normal':  # Skip normal class as it's our reference\n",
    "        print(f\"\\nProcessing attack type: {attack_label}\")\n",
    "        \n",
    "        # Create binary labels for this attack type\n",
    "        y_binary = (y_label_all_encoded == encoded_label).astype(int)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_all, y_binary, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        results = train_evaluate_mlp(X_train, X_test, y_train, y_test)\n",
    "        results['data'] = 'PHY'\n",
    "        results['model_type'] = 'mlp'\n",
    "        results['attack_type'] = attack_label\n",
    "        multiclass_results[attack_label] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les résultats de la classification multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attack_metrics(metric_name):\n",
    "    attacks = list(multiclass_results.keys())\n",
    "    values = [results[metric_name] for results in multiclass_results.values()]\n",
    "    \n",
    "    fig = go.Figure(data=[go.Bar(x=attacks, y=values)])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{metric_name} by Attack Type',\n",
    "        yaxis_title='Score',\n",
    "        xaxis_title='Attack Type'\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "# Plot key metrics\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "for metric in metrics_to_plot:\n",
    "    plot_attack_metrics(metric).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for each attack type\n",
    "for attack_type, results in multiclass_results.items():\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=results['confusion_matrix'],\n",
    "        x=['Predicted Negative', 'Predicted Positive'],\n",
    "        y=['Actual Negative', 'Actual Positive'],\n",
    "        text=results['confusion_matrix'],\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 16},\n",
    "        colorscale='Blues'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Confusion Matrix - {attack_type}',\n",
    "        height=400,\n",
    "        width=500\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for attack_type, results in multiclass_results.items():\n",
    "    summary_data.append({\n",
    "        'Attack Type': attack_type,\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1 Score': results['f1'],\n",
    "        'Training Time (s)': results['fit_time'],\n",
    "        'Prediction Time (s)': results['predict_time'],\n",
    "        'Training Memory (MB)': results['fit_memory_usage'],\n",
    "        'Prediction Memory (MB)': results['predict_memory_usage']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.round(4)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des performances de calcul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot computational performance metrics\n",
    "performance_fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=('Training and Prediction Time', 'Memory Usage')\n",
    ")\n",
    "\n",
    "# Time metrics\n",
    "attacks = list(multiclass_results.keys())\n",
    "train_times = [results['fit_time'] for results in multiclass_results.values()]\n",
    "predict_times = [results['predict_time'] for results in multiclass_results.values()]\n",
    "\n",
    "performance_fig.add_trace(\n",
    "    go.Bar(name='Training Time', x=attacks, y=train_times),\n",
    "    row=1, col=1\n",
    ")\n",
    "performance_fig.add_trace(\n",
    "    go.Bar(name='Prediction Time', x=attacks, y=predict_times),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Memory metrics\n",
    "train_memory = [results['fit_memory_usage'] for results in multiclass_results.values()]\n",
    "predict_memory = [results['predict_memory_usage'] for results in multiclass_results.values()]\n",
    "\n",
    "performance_fig.add_trace(\n",
    "    go.Bar(name='Training Memory', x=attacks, y=train_memory),\n",
    "    row=2, col=1\n",
    ")\n",
    "performance_fig.add_trace(\n",
    "    go.Bar(name='Prediction Memory', x=attacks, y=predict_memory),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "performance_fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Computational Performance by Attack Type\",\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "performance_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarder les résultats pour Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save binary classification results\n",
    "db['PHY_results_mlp_labeln'] = binary_results\n",
    "\n",
    "# Save multiclass results\n",
    "for attack_label in multiclass_results.keys():\n",
    "    db[f'PHY_results_mlp_{attack_label}'] = multiclass_results[attack_label]\n",
    "\n",
    "# Save summary statistics\n",
    "db['mlp_summary_stats'] = {\n",
    "    'summary_df': summary_df,\n",
    "    'binary_results': binary_results,\n",
    "    'multiclass_results': multiclass_results\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_protection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
