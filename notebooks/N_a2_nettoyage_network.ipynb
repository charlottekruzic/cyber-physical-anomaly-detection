{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage des données network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook ne contient que la préparation des données, afin d'obtenir les données préparées plus rapidement.  \n",
    "C'est un copier-coller des parties Nettoyage du notebook `no_run_preparation_network.ipynb`, sans les vérifications.  \n",
    "Les explications des choix de préparation sont dans l'autre notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pickleshare import PickleShareDB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../prep_data' \n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "db = PickleShareDB(os.path.join(data_dir, 'kity'))\n",
    "\n",
    "df_all = [db['net_attack_1'], db['net_attack_2'], db['net_attack_3'], db['net_attack_4'], db['net_norm']]\n",
    "df_nom = ['df_net_1', 'df_net_2', 'df_net_3', 'df_net_4', 'df_net_norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonisation noms colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_all:\n",
    "    df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordre colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = ['Time', 'mac_s', 'mac_d', 'ip_s', 'ip_d', 'sport', 'dport', 'proto', 'flags', 'size', 'modbus_fn', 'n_pkt_src', 'n_pkt_dst', 'modbus_response', 'label_n', 'label']\n",
    "df_all = [df[columns_order] for df in df_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion des lignes dupliquées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous supprimons les lignes dupliqués, et ajoutons une colonne pour informer que ce sont des lignes dupliquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Local\\Temp\\ipykernel_28068\\2598439836.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_all[i].loc[:, 'is_duplicate'] = df_all[i].duplicated(keep=False)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_all)):\n",
    "    df_all[i].loc[:, 'is_duplicate'] = df_all[i].duplicated(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_all)):\n",
    "    df_all[i] = df_all[i].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification des types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(df_all):\n",
    "    df['label_n'] = df['label_n'].astype('bool')\n",
    "    df['label'] = df['label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['sport', 'dport', 'flags', 'n_pkt_src', 'n_pkt_dst']\n",
    "\n",
    "for i in range(len(df_all)):\n",
    "    for col in columns_to_convert:\n",
    "        df_all[i][col] = df_all[i][col].astype('Int64') # ok pour les valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_all)):\n",
    "    df_all[i]['Time'] = pd.to_datetime(df_all[i]['Time'], format='mixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrétisation des ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sports_connus = set(df_all[0]['sport'].dropna().unique())\n",
    "#dports_connus = set(df_all[0]['dport'].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def est_sport_connu(port):\n",
    "#    return port in sports_connus\n",
    "\n",
    "#def est_dport_connu(port):\n",
    "#    return port in dports_connus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for i in range(len(df_all)):\n",
    "#    df_all[i]['est_connu_sport'] = df_all[i]['sport'].apply(est_sport_connu)\n",
    "#    df_all[i]['est_connu_dport'] = df_all[i]['dport'].apply(est_dport_connu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(df_all)):\n",
    "#    print(f\"{df_nom[i]} :\")\n",
    "#    print(df_all[i]['est_connu_sport'].value_counts())\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(df_all)):    \n",
    "#    df_all[i] = df_all[i].drop(columns=['sport', 'dport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorie_ports(port):\n",
    "    if pd.isna(port):\n",
    "        return \"inconnu\" # remplacement des valeurs manquantes\n",
    "    if port < 1024:\n",
    "        return \"system_ports\"\n",
    "    elif port <= 49151:\n",
    "        return \"user_ports\"\n",
    "    else:\n",
    "        return \"dynamic/private_ports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_all)):\n",
    "    for col in ['sport', 'dport']:\n",
    "        df_all[i][col] = df_all[i][col].apply(get_categorie_ports).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation des flags en catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(df_all):\n",
    "    df['flags'] = df['flags'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot de la colonne flags (faire plus tard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(df_all)):\n",
    "#    df_all[i] = pd.get_dummies(df_all[i], columns=['flags'], prefix='flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes créée ne sont pas forcément les même en fonction des datasets, car toutes les valeurs n'étaient pas présentent dans tous les datasets.  \n",
    "Nous ajoutons donc des colonnes avec False si elles n'y sont pas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout des colonnes non créées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_columns = set()\n",
    "#for df in df_all:\n",
    "#    flag_columns = [col for col in df.columns if col.startswith('flag')]\n",
    "#    all_columns.update(flag_columns)\n",
    "#all_columns = list(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(df_all)):\n",
    "#    manquantes = set(all_columns) - set(df_all[i].columns)\n",
    "#    \n",
    "#    for col in manquantes:\n",
    "#        df_all[i][col] = False\n",
    "#    \n",
    "#    df_all[i] = df_all[i].reindex(columns=list(df_all[i].columns) + [col for col in all_columns if col not in df_all[i].columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check si les flags avec des valeurs manquantes ont bien des False dans toutes les colonnes\n",
    "#columns_to_check = ['flag_0', 'flag_10100', 'flag_10', 'flag_10000', 'flag_11000010', 'flag_11001', 'flag_11000', 'flag_1', 'flag_101001', 'flag_10010', 'flag_100', 'flag_10001']\n",
    "#\n",
    "#for i in range(len(df_all)):\n",
    "#    df = df_all[i]\n",
    "#    false_count = (~df[columns_to_check].any(axis=1)).sum()\n",
    "#    print(f\"{df_nom[i]} : {false_count} = False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_all:\n",
    "    for col in ['ip_s', 'ip_d', 'modbus_fn', 'modbus_response']:\n",
    "        df[col] = df[col].fillna(\"inconnue\")\n",
    "    for col in ['n_pkt_src', 'n_pkt_dst']: # La colonne redevient de type float\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    for col in ['flags']:\n",
    "        df[col] = df[col].cat.add_categories('inconnue')\n",
    "        df[col] = df[col].fillna('inconnue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_all:\n",
    "    df['modbus_response'] = df['modbus_response'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "    threshold = 0.001 * len(df)\n",
    "    value_counts = df['modbus_response'].value_counts()\n",
    "    frequent_categories = value_counts[value_counts > threshold].index\n",
    "    df['modbus_response'] = np.where(df['modbus_response'].isin(frequent_categories), df['modbus_response'], 'autre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrement dans PickleShare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../prep_data' \n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "db = PickleShareDB(os.path.join(data_dir, 'kity'))\n",
    "\n",
    "db['net_attack_1_clean'] = df_all[0]\n",
    "db['net_attack_2_clean'] = df_all[1]\n",
    "db['net_attack_3_clean'] = df_all[2]\n",
    "db['net_attack_4_clean'] = df_all[3]\n",
    "db['net_norm_clean'] = df_all[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
