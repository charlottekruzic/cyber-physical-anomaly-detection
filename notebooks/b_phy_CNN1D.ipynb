{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 11:10:19.515330: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-15 11:10:19.523481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731665419.532848   45038 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731665419.535856   45038 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-15 11:10:19.545551: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pickleshare import PickleShareDB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, balanced_accuracy_score\n",
    "import time\n",
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# Pour garantir la reproductibilité de TensorFlow\n",
    "tf.random.set_seed(42)  # Pour TensorFlow\n",
    "np.random.seed(42)  # Pour numpy\n",
    "import random\n",
    "random.seed(42)  # Pour random (Python standard library)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger les données de prep_data\n",
    "\n",
    "data_dir = '../prep_data' \n",
    "db = PickleShareDB(os.path.join(data_dir, 'kity'))\n",
    "\n",
    "if 'df_phy_1' in db:\n",
    "    df_phy_1 = db['df_phy_1']\n",
    "else:\n",
    "    print(\"df_phy_1 n'est pas trouvé dans la base de données.\")\n",
    "if 'df_phy_2' in db:\n",
    "    df_phy_2 = db['df_phy_2']\n",
    "else:\n",
    "    print(\"df_phy_2 n'est pas trouvé dans la base de données.\")\n",
    "if 'df_phy_3' in db:\n",
    "    df_phy_3 = db['df_phy_3']\n",
    "else:\n",
    "    print(\"df_phy_3 n'est pas trouvé dans la base de données.\")\n",
    "if 'df_phy_4' in db:\n",
    "    df_phy_4 = db['df_phy_4']\n",
    "else:\n",
    "    print(\"df_phy_4 n'est pas trouvé dans la base de données.\")\n",
    "if 'df_phy_norm' in db:\n",
    "    df_phy_norm = db['df_phy_norm']\n",
    "else:\n",
    "    print(\"df_phy_norm n'est pas trouvé dans la base de données.\")\n",
    "if 'df_phy_attack' in db:\n",
    "    df_phy_attack = db['df_phy_attack']\n",
    "if 'df_phy_all' in db:\n",
    "    df_phy_all = db['df_phy_all']\n",
    "else:\n",
    "    print(\"df_phy_all n'est pas trouvé dans la base de données.\")\n",
    "if 'dict_dfs' in db:\n",
    "    dict_dfs = db['dict_dfs']\n",
    "else:\n",
    "    print(\"dict_dfs n'est pas trouvé dans la base de données.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2420 entries, 0 to 2419\n",
      "Data columns (total 29 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Time           2420 non-null   datetime64[ns]\n",
      " 1   Tank_1         2420 non-null   int64         \n",
      " 2   Tank_2         2420 non-null   int64         \n",
      " 3   Tank_3         2420 non-null   int64         \n",
      " 4   Tank_4         2420 non-null   int64         \n",
      " 5   Tank_5         2420 non-null   int64         \n",
      " 6   Tank_6         2420 non-null   int64         \n",
      " 7   Tank_7         2420 non-null   int64         \n",
      " 8   Tank_8         2420 non-null   int64         \n",
      " 9   Pump_1         2420 non-null   bool          \n",
      " 10  Pump_2         2420 non-null   bool          \n",
      " 11  Pump_4         2420 non-null   bool          \n",
      " 12  Pump_5         2420 non-null   bool          \n",
      " 13  Pump_6         2420 non-null   bool          \n",
      " 14  Flow_sensor_1  2420 non-null   category      \n",
      " 15  Flow_sensor_2  2420 non-null   bool          \n",
      " 16  Flow_sensor_4  2420 non-null   int64         \n",
      " 17  Valv_10        2420 non-null   bool          \n",
      " 18  Valv_11        2420 non-null   bool          \n",
      " 19  Valv_12        2420 non-null   bool          \n",
      " 20  Valv_13        2420 non-null   bool          \n",
      " 21  Valv_14        2420 non-null   bool          \n",
      " 22  Valv_15        2420 non-null   bool          \n",
      " 23  Valv_17        2420 non-null   bool          \n",
      " 24  Valv_18        2420 non-null   bool          \n",
      " 25  Valv_20        2420 non-null   bool          \n",
      " 26  Valv_22        2420 non-null   bool          \n",
      " 27  Label_n        2420 non-null   bool          \n",
      " 28  Label          2420 non-null   category      \n",
      "dtypes: bool(17), category(2), datetime64[ns](1), int64(9)\n",
      "memory usage: 234.1 KB\n"
     ]
    }
   ],
   "source": [
    "df_phy_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Tank_1</th>\n",
       "      <th>Tank_2</th>\n",
       "      <th>Tank_3</th>\n",
       "      <th>Tank_4</th>\n",
       "      <th>Tank_5</th>\n",
       "      <th>Tank_6</th>\n",
       "      <th>Tank_7</th>\n",
       "      <th>Tank_8</th>\n",
       "      <th>Pump_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Valv_12</th>\n",
       "      <th>Valv_13</th>\n",
       "      <th>Valv_14</th>\n",
       "      <th>Valv_15</th>\n",
       "      <th>Valv_17</th>\n",
       "      <th>Valv_18</th>\n",
       "      <th>Valv_20</th>\n",
       "      <th>Valv_22</th>\n",
       "      <th>Label_n</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-09 18:23:28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-09 18:23:29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-09 18:23:30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-09 18:23:31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-09 18:23:32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Time  Tank_1  Tank_2  Tank_3  Tank_4  Tank_5  Tank_6  Tank_7  \\\n",
       "0 2021-04-09 18:23:28       0       0       0       0       0       0       0   \n",
       "1 2021-04-09 18:23:29       0       0       0       0       0       0       0   \n",
       "2 2021-04-09 18:23:30       0       0       0       0       0       0       0   \n",
       "3 2021-04-09 18:23:31       0       0       0       0       0       0       0   \n",
       "4 2021-04-09 18:23:32       0       0       0       0       0       0       0   \n",
       "\n",
       "   Tank_8  Pump_1  ...  Valv_12  Valv_13  Valv_14  Valv_15 Valv_17  Valv_18  \\\n",
       "0       0   False  ...    False    False    False    False   False    False   \n",
       "1       0   False  ...    False    False    False    False   False    False   \n",
       "2       0   False  ...    False    False    False    False   False    False   \n",
       "3       0   False  ...    False    False    False    False   False    False   \n",
       "4       0    True  ...    False    False    False    False   False    False   \n",
       "\n",
       "   Valv_20  Valv_22  Label_n   Label  \n",
       "0    False    False    False  normal  \n",
       "1    False    False    False  normal  \n",
       "2    False    False    False  normal  \n",
       "3    False    False    False  normal  \n",
       "4    False    False    False  normal  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_phy_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 séries temporelles multivariées\n",
    "# créer des fenêtres temporelles sur chaque jeu de données sans essayer de les fusionner sur les dates \n",
    "# chaque dataset aura ses propres fenêtres glissantes\n",
    "# supprimer la colonne de temps\n",
    "# ensuite diviser en X et y (Label et Label_n, on chercher à prédire l'un ou l'autre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1D_df_phy_1 = df_phy_1.drop(columns=['Time'])\n",
    "cnn_1D_df_phy_2 = df_phy_2.drop(columns=['Time'])\n",
    "cnn_1D_df_phy_3 = df_phy_3.drop(columns=['Time'])\n",
    "cnn_1D_df_phy_4 = df_phy_4.drop(columns=['Time'])\n",
    "cnn_1D_df_phy_norm = df_phy_norm.drop(columns=['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser en X (features) et y (labels)\n",
    "X_phy_1 = cnn_1D_df_phy_1.drop(columns=['Label', 'Label_n'])  \n",
    "y_label_phy_1 = cnn_1D_df_phy_1['Label']                      \n",
    "y_label_n_phy_1 = cnn_1D_df_phy_1['Label_n']                  \n",
    "\n",
    "X_phy_2 = cnn_1D_df_phy_2.drop(columns=['Label', 'Label_n'])  \n",
    "y_label_phy_2 = cnn_1D_df_phy_2['Label']                      \n",
    "y_label_n_phy_2 = cnn_1D_df_phy_2['Label_n']      \n",
    "\n",
    "X_phy_3 = cnn_1D_df_phy_3.drop(columns=['Label', 'Label_n'])\n",
    "y_label_phy_3 = cnn_1D_df_phy_3['Label']\n",
    "y_label_n_phy_3 = cnn_1D_df_phy_3['Label_n']\n",
    "\n",
    "X_phy_4 = cnn_1D_df_phy_4.drop(columns=['Label', 'Label_n'])\n",
    "y_label_phy_4 = cnn_1D_df_phy_4['Label']\n",
    "y_label_n_phy_4 = cnn_1D_df_phy_4['Label_n']\n",
    "\n",
    "X_phy_norm = cnn_1D_df_phy_norm.drop(columns=['Label', 'Label_n'])\n",
    "y_label_phy_norm = cnn_1D_df_phy_norm['Label']\n",
    "y_label_n_phy_norm = cnn_1D_df_phy_norm['Label_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : pas sur de ce coup là; je pense l'enlever : à voir\n",
    "\n",
    "# on va concaténer les données, mais la fin d'un dataset n'est pas lié au début du suivant\n",
    "# on ajoute juste une colonne pour identifier le dataset d'origine\n",
    "X_phy_1['dataset_id'] = 'dataset_1'\n",
    "X_phy_2['dataset_id'] = 'dataset_2'\n",
    "X_phy_3['dataset_id'] = 'dataset_3'\n",
    "X_phy_4['dataset_id'] = 'dataset_4'\n",
    "X_phy_norm['dataset_id'] = 'dataset_norm'\n",
    "\n",
    "X_phy_1['dataset_id'] = X_phy_1['dataset_id'].astype('category')\n",
    "X_phy_2['dataset_id'] = X_phy_2['dataset_id'].astype('category')\n",
    "X_phy_3['dataset_id'] = X_phy_3['dataset_id'].astype('category')\n",
    "X_phy_4['dataset_id'] = X_phy_4['dataset_id'].astype('category')\n",
    "X_phy_norm['dataset_id'] = X_phy_norm['dataset_id'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliser : MinMaxScaler nous permet de préparer nos données de manière efficace,\n",
    "# garantissant que chaque feature contribut de manière égale au processus d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour normaliser uniquement les colonnes numériques d'un DataFrame\n",
    "def normalize_numeric_columns(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_cols = df.select_dtypes(include=['int64']).columns  # Sélectionne les colonnes numériques\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])  # Applique le scaler uniquement sur les colonnes numériques\n",
    "    return df\n",
    "\n",
    "# Appliquez la normalisation sur chaque DataFrame individuellement\n",
    "X_phy_1 = normalize_numeric_columns(X_phy_1)\n",
    "X_phy_2 = normalize_numeric_columns(X_phy_2)\n",
    "X_phy_3 = normalize_numeric_columns(X_phy_3)\n",
    "X_phy_4 = normalize_numeric_columns(X_phy_4)\n",
    "X_phy_norm = normalize_numeric_columns(X_phy_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Flow_sensor_1', 'dataset_id'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = [df.select_dtypes(include=['category']).columns for df in [X_phy_1, X_phy_2, X_phy_3, X_phy_4, X_phy_norm]]  \n",
    "cat_cols = set(cat_cols[0])  \n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Flow_sensor_1': [0, 100, 4000],\n",
       " 'dataset_id': ['dataset_norm',\n",
       "  'dataset_2',\n",
       "  'dataset_3',\n",
       "  'dataset_1',\n",
       "  'dataset_4']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extraire tous les catégories pour les colonnes catégorielles \n",
    "\n",
    "def extract_categories(df):\n",
    "    return {col: list(df[col].cat.categories) for col in df.select_dtypes(include=['category']).columns}\n",
    "\n",
    "cat_dict_phy_1 = extract_categories(X_phy_1)\n",
    "cat_dict_phy_2 = extract_categories(X_phy_2)\n",
    "cat_dict_phy_3 = extract_categories(X_phy_3)\n",
    "cat_dict_phy_4 = extract_categories(X_phy_4)\n",
    "cat_dict_phy_norm = extract_categories(X_phy_norm)\n",
    "\n",
    "# fusionner les catégories de tous les datasets\n",
    "cat_dict_all = {}\n",
    "for cat_dict in [cat_dict_phy_1, cat_dict_phy_2, cat_dict_phy_3, cat_dict_phy_4, cat_dict_phy_norm]:\n",
    "    for key, value in cat_dict.items():\n",
    "        if key not in cat_dict_all:\n",
    "            cat_dict_all[key] = value\n",
    "        else:\n",
    "            cat_dict_all[key] = list(set(cat_dict_all[key] + value))\n",
    "cat_dict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour appliquer le One-Hot Encoding tout en gardant les mêmes colonnes\n",
    "def apply_one_hot_encoding(df, cat_dict_all):\n",
    "    for col, categories in cat_dict_all.items():\n",
    "        if col in df.columns:  # Appliquer le One-Hot uniquement aux colonnes présentes\n",
    "            # Convertir la colonne en Categorical avec les catégories globales\n",
    "            df[col] = pd.Categorical(df[col], categories=categories)\n",
    "            \n",
    "            # Créer un DataFrame avec le One-Hot Encoding en utilisant les catégories globales\n",
    "            dummies = pd.get_dummies(df[col], prefix=col)\n",
    "            \n",
    "            # Ajouter les colonnes dummies et supprimer la colonne originale\n",
    "            df = pd.concat([df, dummies], axis=1).drop(col, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Appliquer One-Hot Encoding pour chaque dataframe\n",
    "X_phy_1 = apply_one_hot_encoding(X_phy_1, cat_dict_all)\n",
    "X_phy_2 = apply_one_hot_encoding(X_phy_2, cat_dict_all)\n",
    "X_phy_3 = apply_one_hot_encoding(X_phy_3, cat_dict_all)\n",
    "X_phy_4 = apply_one_hot_encoding(X_phy_4, cat_dict_all)\n",
    "X_phy_norm = apply_one_hot_encoding(X_phy_norm, cat_dict_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2420, 33)\n",
      "(2104, 33)\n",
      "(1254, 33)\n",
      "(1717, 33)\n",
      "(3428, 33)\n"
     ]
    }
   ],
   "source": [
    "# vérif de la taille des données\n",
    "print(X_phy_1.shape)\n",
    "print(X_phy_2.shape)\n",
    "print(X_phy_3.shape)\n",
    "print(X_phy_4.shape)\n",
    "print(X_phy_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2420 entries, 0 to 2419\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Tank_1                   2420 non-null   float64\n",
      " 1   Tank_2                   2420 non-null   float64\n",
      " 2   Tank_3                   2420 non-null   float64\n",
      " 3   Tank_4                   2420 non-null   float64\n",
      " 4   Tank_5                   2420 non-null   float64\n",
      " 5   Tank_6                   2420 non-null   float64\n",
      " 6   Tank_7                   2420 non-null   float64\n",
      " 7   Tank_8                   2420 non-null   float64\n",
      " 8   Pump_1                   2420 non-null   bool   \n",
      " 9   Pump_2                   2420 non-null   bool   \n",
      " 10  Pump_4                   2420 non-null   bool   \n",
      " 11  Pump_5                   2420 non-null   bool   \n",
      " 12  Pump_6                   2420 non-null   bool   \n",
      " 13  Flow_sensor_2            2420 non-null   bool   \n",
      " 14  Flow_sensor_4            2420 non-null   float64\n",
      " 15  Valv_10                  2420 non-null   bool   \n",
      " 16  Valv_11                  2420 non-null   bool   \n",
      " 17  Valv_12                  2420 non-null   bool   \n",
      " 18  Valv_13                  2420 non-null   bool   \n",
      " 19  Valv_14                  2420 non-null   bool   \n",
      " 20  Valv_15                  2420 non-null   bool   \n",
      " 21  Valv_17                  2420 non-null   bool   \n",
      " 22  Valv_18                  2420 non-null   bool   \n",
      " 23  Valv_20                  2420 non-null   bool   \n",
      " 24  Valv_22                  2420 non-null   bool   \n",
      " 25  Flow_sensor_1_0          2420 non-null   bool   \n",
      " 26  Flow_sensor_1_100        2420 non-null   bool   \n",
      " 27  Flow_sensor_1_4000       2420 non-null   bool   \n",
      " 28  dataset_id_dataset_norm  2420 non-null   bool   \n",
      " 29  dataset_id_dataset_2     2420 non-null   bool   \n",
      " 30  dataset_id_dataset_3     2420 non-null   bool   \n",
      " 31  dataset_id_dataset_1     2420 non-null   bool   \n",
      " 32  dataset_id_dataset_4     2420 non-null   bool   \n",
      "dtypes: bool(24), float64(9)\n",
      "memory usage: 227.0 KB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3428 entries, 0 to 3427\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Tank_1                   3428 non-null   float64\n",
      " 1   Tank_2                   3428 non-null   float64\n",
      " 2   Tank_3                   3428 non-null   float64\n",
      " 3   Tank_4                   3428 non-null   float64\n",
      " 4   Tank_5                   3428 non-null   float64\n",
      " 5   Tank_6                   3428 non-null   float64\n",
      " 6   Tank_7                   3428 non-null   float64\n",
      " 7   Tank_8                   3428 non-null   float64\n",
      " 8   Pump_1                   3428 non-null   bool   \n",
      " 9   Pump_2                   3428 non-null   bool   \n",
      " 10  Pump_4                   3428 non-null   bool   \n",
      " 11  Pump_5                   3428 non-null   bool   \n",
      " 12  Pump_6                   3428 non-null   bool   \n",
      " 13  Flow_sensor_2            3428 non-null   bool   \n",
      " 14  Flow_sensor_4            3428 non-null   float64\n",
      " 15  Valv_10                  3428 non-null   bool   \n",
      " 16  Valv_11                  3428 non-null   bool   \n",
      " 17  Valv_12                  3428 non-null   bool   \n",
      " 18  Valv_13                  3428 non-null   bool   \n",
      " 19  Valv_14                  3428 non-null   bool   \n",
      " 20  Valv_15                  3428 non-null   bool   \n",
      " 21  Valv_17                  3428 non-null   bool   \n",
      " 22  Valv_18                  3428 non-null   bool   \n",
      " 23  Valv_20                  3428 non-null   bool   \n",
      " 24  Valv_22                  3428 non-null   bool   \n",
      " 25  Flow_sensor_1_0          3428 non-null   bool   \n",
      " 26  Flow_sensor_1_100        3428 non-null   bool   \n",
      " 27  Flow_sensor_1_4000       3428 non-null   bool   \n",
      " 28  dataset_id_dataset_norm  3428 non-null   bool   \n",
      " 29  dataset_id_dataset_2     3428 non-null   bool   \n",
      " 30  dataset_id_dataset_3     3428 non-null   bool   \n",
      " 31  dataset_id_dataset_1     3428 non-null   bool   \n",
      " 32  dataset_id_dataset_4     3428 non-null   bool   \n",
      "dtypes: bool(24), float64(9)\n",
      "memory usage: 321.5 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# vérif info\n",
    "print(X_phy_1.info())\n",
    "print()\n",
    "print(X_phy_norm.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow_sensor_1_100\n",
      "False    2104\n",
      "Name: count, dtype: int64\n",
      "dataset_id_dataset_1\n",
      "False    1254\n",
      "Name: count, dtype: int64\n",
      "dataset_id_dataset_3\n",
      "True    1254\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_phy_2.value_counts('Flow_sensor_1_100'))\n",
    "# que des False -> ok\n",
    "\n",
    "print(X_phy_3.value_counts('dataset_id_dataset_1'))\n",
    "# que False -> ok\n",
    "\n",
    "print(X_phy_3.value_counts('dataset_id_dataset_3'))\n",
    "# que True -> ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction pour récupérer les valeurs uniques de chaque colonne catégorielle à travers tous les datasets\n",
    "def get_unique_values_all_labels(*dfs):\n",
    "    unique_values = {}\n",
    "    \n",
    "    # Combiner tous les datasets\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Identifier les colonnes catégorielles\n",
    "    for col in combined_df.select_dtypes(include=['object']).columns:\n",
    "        # Ajouter les valeurs uniques de chaque colonne catégorielle\n",
    "        unique_values[col] = combined_df[col].unique()\n",
    "    \n",
    "    return unique_values\n",
    "\n",
    "unique_values = get_unique_values_all_labels(X_phy_1, X_phy_2, X_phy_3, X_phy_4, X_phy_norm)\n",
    "unique_values\n",
    "\n",
    "# plus rien -> ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "normal            1610\n",
      "MITM               533\n",
      "physical fault     277\n",
      "Name: count, dtype: int64\n",
      "Label\n",
      "normal            1798\n",
      "physical fault     123\n",
      "MITM                96\n",
      "DoS                 80\n",
      "scan                 7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_label_phy_1.value_counts())\n",
    "print(y_label_phy_2.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "normal            8906\n",
      "MITM              1008\n",
      "physical fault     685\n",
      "DoS                310\n",
      "scan                14\n",
      "Name: count, dtype: int64\n",
      "{np.str_('normal'): 0, np.str_('DoS'): 1, np.str_('MITM'): 2, np.str_('physical fault'): 3, np.str_('scan'): 4}\n"
     ]
    }
   ],
   "source": [
    "# Concaténer les labels de tous les datasets\n",
    "all_labels = pd.concat([y_label_phy_1, y_label_phy_2, y_label_phy_3, y_label_phy_4, y_label_phy_norm], ignore_index=True)\n",
    "print(all_labels.value_counts())\n",
    "\n",
    "# Définir explicitement l'ordre des classes, avec 'normal' en premier\n",
    "ordered_classes = ['normal', 'DoS', 'MITM', 'physical fault', 'scan']\n",
    "nb_class = 5\n",
    "\n",
    "# Créer un LabelEncoder et assigner directement l'ordre des classes\n",
    "label_encoder = LabelEncoder()\n",
    "# Assigner les classes manuellement : on force l'ordre pour que normal soit à 0\n",
    "label_encoder.classes_ = np.array(ordered_classes)\n",
    "\n",
    "# Récupérer la correspondance entre les labels d'origine et les labels encodés\n",
    "label_mapping = {label: encoded for label, encoded in zip(label_encoder.classes_, range(len(label_encoder.classes_)))}\n",
    "print(label_mapping)\n",
    "\n",
    "# Appliquer la transformation sur les datasets\n",
    "y_label_phy_1_encoder = label_encoder.transform(df_phy_1['Label'])\n",
    "y_label_phy_2_encoder = label_encoder.transform(df_phy_2['Label'])\n",
    "y_label_phy_3_encoder = label_encoder.transform(df_phy_3['Label'])\n",
    "y_label_phy_4_encoder = label_encoder.transform(df_phy_4['Label'])\n",
    "y_label_phy_norm_encoder = label_encoder.transform(df_phy_norm['Label'])\n",
    "\n",
    "# Convertir en DataFrame\n",
    "y_label_phy_1_df_enc = pd.DataFrame(y_label_phy_1_encoder, columns=['encoded_label'])\n",
    "y_label_phy_2_df_enc = pd.DataFrame(y_label_phy_2_encoder, columns=['encoded_label'])\n",
    "y_label_phy_3_df_enc = pd.DataFrame(y_label_phy_3_encoder, columns=['encoded_label'])\n",
    "y_label_phy_4_df_enc = pd.DataFrame(y_label_phy_4_encoder, columns=['encoded_label'])\n",
    "y_label_phy_norm_df_enc = pd.DataFrame(y_label_phy_norm_encoder, columns=['encoded_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "normal            1610\n",
      "MITM               533\n",
      "physical fault     277\n",
      "Name: count, dtype: int64\n",
      "encoded_label\n",
      "0    1610\n",
      "2     533\n",
      "3     277\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Affichage pour vérifier\n",
    "print(y_label_phy_1.value_counts())\n",
    "print(y_label_phy_1_df_enc['encoded_label'].value_counts())\n",
    "# ok -> on écrase y_label_phy_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_phy_1 = y_label_phy_1_df_enc\n",
    "y_label_phy_2 = y_label_phy_2_df_enc\n",
    "y_label_phy_3 = y_label_phy_3_df_enc\n",
    "y_label_phy_4 = y_label_phy_4_df_enc\n",
    "y_label_phy_norm = y_label_phy_norm_df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2420, 33)\n",
      "(2420,)\n",
      "(2420, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_phy_1.shape)\n",
    "print(y_label_n_phy_1.shape)\n",
    "print(y_label_phy_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation pour essayer de détecter les attaques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction de Label_n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(X, y, window_size):\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "\n",
    "    for i in range(len(X) - window_size):\n",
    "        X_windows.append(X.iloc[i:i + window_size])  \n",
    "        y_windows.append(y.iloc[i + window_size])   \n",
    "\n",
    "    return np.array(X_windows), np.array(y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10 # TODO : déterminer 10 secondes via viz ? sinon changer la valeur\n",
    "\n",
    "# Créer des fenêtres glissantes pour X et y\n",
    "X_windows_phy_1, y_windows_label_n_phy_1 = create_sliding_windows(X_phy_1, y_label_n_phy_1, window_size)\n",
    "X_windows_phy_2, y_windows_label_n_phy_2 = create_sliding_windows(X_phy_2, y_label_n_phy_2, window_size)\n",
    "X_windows_phy_3, y_windows_label_n_phy_3 = create_sliding_windows(X_phy_3, y_label_n_phy_3, window_size)\n",
    "X_windows_phy_4, y_windows_label_n_phy_4 = create_sliding_windows(X_phy_4, y_label_n_phy_4, window_size)\n",
    "X_windows_phy_norm, y_windows_label_n_phy_norm = create_sliding_windows(X_phy_norm, y_label_n_phy_norm, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2410, 10, 33)\n",
      "(2410,)\n",
      "(2094, 10, 33)\n",
      "(2094,)\n",
      "(1244, 10, 33)\n",
      "(1244,)\n",
      "(1707, 10, 33)\n",
      "(1707,)\n",
      "(3418, 10, 33)\n",
      "(3418,)\n"
     ]
    }
   ],
   "source": [
    "# Vérifier la forme des données\n",
    "print(X_windows_phy_1.shape)  # (nombre d'échantillons, window_size, nombre de features)\n",
    "print(y_windows_label_n_phy_1.shape)  # (nombre d'échantillons,)\n",
    "\n",
    "print(X_windows_phy_2.shape)\n",
    "print(y_windows_label_n_phy_2.shape)\n",
    "\n",
    "print(X_windows_phy_3.shape)\n",
    "print(y_windows_label_n_phy_3.shape)\n",
    "\n",
    "print(X_windows_phy_4.shape)\n",
    "print(y_windows_label_n_phy_4.shape)\n",
    "\n",
    "print(X_windows_phy_norm.shape)\n",
    "print(y_windows_label_n_phy_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_all: (10873, 10, 33)\n",
      "Shape of y_all: (10873,)\n"
     ]
    }
   ],
   "source": [
    "# concaténer les données\n",
    "\n",
    "X_all = np.concatenate([X_windows_phy_1, X_windows_phy_2, X_windows_phy_3, X_windows_phy_4, X_windows_phy_norm], axis=0)\n",
    "y_all = np.concatenate([y_windows_label_n_phy_1, y_windows_label_n_phy_2, y_windows_label_n_phy_3, y_windows_label_n_phy_4, y_windows_label_n_phy_norm], axis=0)\n",
    "\n",
    "# Vérifier la forme des données après concaténation\n",
    "print(\"Shape of X_all:\", X_all.shape)  # (nombre d'échantillons, window_size, nombre de features)\n",
    "print(\"Shape of y_all:\", y_all.shape)  # (nombre d'échantillons,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les colonnes booléennes de X_train et X_test en float32\n",
    "X_all = X_all.astype('float64')\n",
    "y_all = y_all.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# division en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> float64\n",
      "<class 'numpy.ndarray'> float64\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), X_train.dtype)\n",
    "print(type(y_train), y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zoe/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-11-15 11:10:21.556453: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Définir et entraîner le modèle CNN 1D\n",
    "model = Sequential()\n",
    "\n",
    "# Ajouter une couche Conv1D\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Ajouter d'autres couches convolutives si nécessaire\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Aplatir les sorties pour les couches fully connected\n",
    "model.add(Flatten())\n",
    "\n",
    "# Ajouter une couche Dense pour la classification binaire\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Une sortie avec activation sigmoïde pour classification binaire\n",
    "\n",
    "# Compiler le modèle : binary_crossentropy pour une classification binaire\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mise en place de l'EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8322 - loss: 0.3858 - val_accuracy: 0.9016 - val_loss: 0.2415\n",
      "Epoch 2/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8933 - loss: 0.2324 - val_accuracy: 0.9131 - val_loss: 0.2000\n",
      "Epoch 3/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9174 - loss: 0.1825 - val_accuracy: 0.9297 - val_loss: 0.1628\n",
      "Epoch 4/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9291 - loss: 0.1523 - val_accuracy: 0.9402 - val_loss: 0.1330\n",
      "Epoch 5/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9350 - loss: 0.1358 - val_accuracy: 0.9476 - val_loss: 0.1157\n",
      "Epoch 6/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9438 - loss: 0.1194 - val_accuracy: 0.9531 - val_loss: 0.1056\n",
      "Epoch 7/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9510 - loss: 0.1092 - val_accuracy: 0.9563 - val_loss: 0.0975\n",
      "Epoch 8/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9571 - loss: 0.1010 - val_accuracy: 0.9632 - val_loss: 0.0856\n",
      "Epoch 9/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9601 - loss: 0.0948 - val_accuracy: 0.9572 - val_loss: 0.0934\n",
      "Epoch 10/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9603 - loss: 0.0881 - val_accuracy: 0.9637 - val_loss: 0.0869\n",
      "Epoch 11/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9602 - loss: 0.0855 - val_accuracy: 0.9609 - val_loss: 0.0956\n",
      "Epoch 12/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9640 - loss: 0.0847 - val_accuracy: 0.9729 - val_loss: 0.0785\n",
      "Epoch 13/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9648 - loss: 0.0752 - val_accuracy: 0.9646 - val_loss: 0.0849\n",
      "Epoch 14/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9652 - loss: 0.0746 - val_accuracy: 0.9720 - val_loss: 0.0813\n",
      "Epoch 15/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9682 - loss: 0.0720 - val_accuracy: 0.9651 - val_loss: 0.1074\n",
      "Epoch 16/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9686 - loss: 0.0723 - val_accuracy: 0.9632 - val_loss: 0.0924\n",
      "Epoch 17/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9707 - loss: 0.0660 - val_accuracy: 0.9697 - val_loss: 0.0725\n",
      "Epoch 18/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9712 - loss: 0.0669 - val_accuracy: 0.9641 - val_loss: 0.0920\n",
      "Epoch 19/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9749 - loss: 0.0614 - val_accuracy: 0.9710 - val_loss: 0.0796\n",
      "Epoch 20/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9738 - loss: 0.0609 - val_accuracy: 0.9729 - val_loss: 0.0615\n",
      "Epoch 21/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9734 - loss: 0.0623 - val_accuracy: 0.9793 - val_loss: 0.0500\n",
      "Epoch 22/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9781 - loss: 0.0548 - val_accuracy: 0.9784 - val_loss: 0.0561\n",
      "Epoch 23/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9810 - loss: 0.0511 - val_accuracy: 0.9752 - val_loss: 0.0611\n",
      "Epoch 24/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9805 - loss: 0.0474 - val_accuracy: 0.9752 - val_loss: 0.0571\n",
      "Epoch 25/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9818 - loss: 0.0459 - val_accuracy: 0.9775 - val_loss: 0.0677\n",
      "Epoch 26/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9808 - loss: 0.0454 - val_accuracy: 0.9811 - val_loss: 0.0546\n",
      "Epoch 27/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9808 - loss: 0.0450 - val_accuracy: 0.9738 - val_loss: 0.0817\n",
      "Epoch 28/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9817 - loss: 0.0519 - val_accuracy: 0.9756 - val_loss: 0.0612\n",
      "Epoch 29/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9838 - loss: 0.0455 - val_accuracy: 0.9789 - val_loss: 0.0594\n",
      "Epoch 30/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9823 - loss: 0.0444 - val_accuracy: 0.9761 - val_loss: 0.0703\n",
      "Epoch 31/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9851 - loss: 0.0424 - val_accuracy: 0.9766 - val_loss: 0.0660\n"
     ]
    }
   ],
   "source": [
    "# Mesurer le temps et la mémoire pour l'entraînement\n",
    "tracemalloc.start()\n",
    "start_fit_time = time.time()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "fit_time = time.time() - start_fit_time\n",
    "current, peak = tracemalloc.get_traced_memory()  # Mémoire actuelle et maximale utilisée\n",
    "fit_memory_usage = peak / (1024 * 1024)  # Convertir en Mo\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Mesurer le temps et la mémoire pour l'entraînement\n",
    "tracemalloc.start()\n",
    "start_predict_time = time.time()\n",
    "\n",
    "# Prédire sur le jeu de test\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()  # Convertir les probabilités en classes binaires (True/False)\n",
    "\n",
    "\n",
    "predict_time = time.time() - start_predict_time\n",
    "current, peak = tracemalloc.get_traced_memory()  # Mémoire actuelle et maximale utilisée\n",
    "predict_memory_usage = peak / (1024 * 1024)  # Convertir en Mo\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les métriques de classification\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel() # attention c'est dans cet ordre !\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall_tpr = recall_score(y_test, y_pred)\n",
    "tnr = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "fpr = FP / (FP + TN) if (FP + TN) != 0 else 0\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "results_cnn1d_label_n = {\n",
    "    'data': 'PHY',\n",
    "    'model_type': 'CNN 1D',\n",
    "    'attack_type': 'labeln',\n",
    "    'confusion_matrix': conf_matrix,\n",
    "    'precision': precision,\n",
    "    'recall': recall_tpr,\n",
    "    'tnr': tnr,\n",
    "    'fpr': fpr,\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'balanced_accuracy': balanced_acc,\n",
    "    'mcc': mcc,\n",
    "    'fit_time': fit_time,\n",
    "    'predict_time': predict_time,\n",
    "    'fit_memory_usage': fit_memory_usage,\n",
    "    'predict_memory_usage': predict_memory_usage\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation complète du modèle:\n",
      "data: PHY\n",
      "model_type: CNN 1D\n",
      "attack_type: labeln\n",
      "confusion_matrix: [[1760   21]\n",
      " [  24  370]]\n",
      "precision: 0.9462915601023018\n",
      "recall: 0.9390862944162437\n",
      "tnr: 0.9882088714205502\n",
      "fpr: 0.011791128579449747\n",
      "accuracy: 0.9793103448275862\n",
      "f1: 0.9426751592356688\n",
      "balanced_accuracy: 0.963647582918397\n",
      "mcc: 0.9300627754599318\n",
      "fit_time: 52.49636673927307\n",
      "predict_time: 0.44004344940185547\n",
      "fit_memory_usage: 22.819459915161133\n",
      "predict_memory_usage: 5.594414710998535\n"
     ]
    }
   ],
   "source": [
    "# Afficher les résultats\n",
    "print(\"Évaluation complète du modèle:\")\n",
    "for metric, value in results_cnn1d_label_n.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder dans PickleShareDB\n",
    "db['PHY_results_cnn1d_labeln'] = results_cnn1d_label_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction de Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10 # TODO : déterminer 10 secondes via viz ? sinon changer la valeur\n",
    "\n",
    "# Créer des fenêtres glissantes pour X et y\n",
    "X_windows_phy_1, y_windows_label_phy_1 = create_sliding_windows(X_phy_1, y_label_phy_1, window_size)\n",
    "X_windows_phy_2, y_windows_label_phy_2 = create_sliding_windows(X_phy_2, y_label_phy_2, window_size)\n",
    "X_windows_phy_3, y_windows_label_phy_3 = create_sliding_windows(X_phy_3, y_label_phy_3, window_size)\n",
    "X_windows_phy_4, y_windows_label_phy_4 = create_sliding_windows(X_phy_4, y_label_phy_4, window_size)\n",
    "X_windows_phy_norm, y_windows_label_phy_norm = create_sliding_windows(X_phy_norm, y_label_phy_norm, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2410, 10, 33)\n",
      "(2410, 1)\n",
      "(2094, 10, 33)\n",
      "(2094, 1)\n",
      "(1244, 10, 33)\n",
      "(1244, 1)\n",
      "(1707, 10, 33)\n",
      "(1707, 1)\n",
      "(3418, 10, 33)\n",
      "(3418, 1)\n"
     ]
    }
   ],
   "source": [
    "# Vérifier la forme des données\n",
    "print(X_windows_phy_1.shape)  # (nombre d'échantillons, window_size, nombre de features)\n",
    "print(y_windows_label_phy_1.shape)  # (nombre d'échantillons,)\n",
    "\n",
    "print(X_windows_phy_2.shape)\n",
    "print(y_windows_label_phy_2.shape)\n",
    "\n",
    "print(X_windows_phy_3.shape)\n",
    "print(y_windows_label_phy_3.shape)\n",
    "\n",
    "print(X_windows_phy_4.shape)\n",
    "print(y_windows_label_phy_4.shape)\n",
    "\n",
    "print(X_windows_phy_norm.shape)\n",
    "print(y_windows_label_phy_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_all: (10873, 10, 33)\n",
      "Shape of y_all: (10873, 1)\n"
     ]
    }
   ],
   "source": [
    "# concaténer les données\n",
    "\n",
    "X_all = np.concatenate([X_windows_phy_1, X_windows_phy_2, X_windows_phy_3, X_windows_phy_4, X_windows_phy_norm], axis=0)\n",
    "y_all = np.concatenate([y_windows_label_phy_1, y_windows_label_phy_2, y_windows_label_phy_3, y_windows_label_phy_4, y_windows_label_phy_norm], axis=0)\n",
    "\n",
    "# Vérifier la forme des données après concaténation\n",
    "print(\"Shape of X_all:\", X_all.shape)  # (nombre d'échantillons, window_size, nombre de features)\n",
    "print(\"Shape of y_all:\", y_all.shape)  # (nombre d'échantillons,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les colonnes booléennes de X_train et X_test en float32\n",
    "X_all = X_all.astype('float64')\n",
    "y_all = y_all.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# division en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zoe/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Définir et entraîner le modèle CNN 1D\n",
    "model = Sequential()\n",
    "\n",
    "# Ajouter une couche Conv1D\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Ajouter d'autres couches convolutives si nécessaire\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Aplatir les sorties pour les couches fully connected\n",
    "model.add(Flatten())\n",
    "\n",
    "# Ajouter une couche Dense pour la classification multiclasse\n",
    "model.add(Dense(nb_class, activation='softmax'))  # Une sortie avec activation softmax pour classification multiclasse\n",
    "\n",
    "# Compiler le modèle : sparse_categorical_crossentropy pour une classification multiclasse\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mise en place de l'EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7824 - loss: 0.6259 - val_accuracy: 0.8883 - val_loss: 0.2896\n",
      "Epoch 2/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8863 - loss: 0.2821 - val_accuracy: 0.9237 - val_loss: 0.2043\n",
      "Epoch 3/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9124 - loss: 0.2064 - val_accuracy: 0.9287 - val_loss: 0.1687\n",
      "Epoch 4/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9270 - loss: 0.1695 - val_accuracy: 0.9370 - val_loss: 0.1434\n",
      "Epoch 5/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9360 - loss: 0.1478 - val_accuracy: 0.9457 - val_loss: 0.1229\n",
      "Epoch 6/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9447 - loss: 0.1324 - val_accuracy: 0.9536 - val_loss: 0.1068\n",
      "Epoch 7/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9482 - loss: 0.1205 - val_accuracy: 0.9605 - val_loss: 0.0977\n",
      "Epoch 8/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9547 - loss: 0.1088 - val_accuracy: 0.9641 - val_loss: 0.0882\n",
      "Epoch 9/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9589 - loss: 0.1040 - val_accuracy: 0.9664 - val_loss: 0.0854\n",
      "Epoch 10/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9648 - loss: 0.0932 - val_accuracy: 0.9664 - val_loss: 0.0816\n",
      "Epoch 11/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.0877 - val_accuracy: 0.9651 - val_loss: 0.0776\n",
      "Epoch 12/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9677 - loss: 0.0836 - val_accuracy: 0.9687 - val_loss: 0.0756\n",
      "Epoch 13/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.0776 - val_accuracy: 0.9729 - val_loss: 0.0696\n",
      "Epoch 14/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9715 - loss: 0.0748 - val_accuracy: 0.9701 - val_loss: 0.0745\n",
      "Epoch 15/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9721 - loss: 0.0727 - val_accuracy: 0.9651 - val_loss: 0.0785\n",
      "Epoch 16/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9731 - loss: 0.0727 - val_accuracy: 0.9710 - val_loss: 0.0760\n",
      "Epoch 17/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.0686 - val_accuracy: 0.9715 - val_loss: 0.0687\n",
      "Epoch 18/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9736 - loss: 0.0686 - val_accuracy: 0.9692 - val_loss: 0.0745\n",
      "Epoch 19/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9750 - loss: 0.0652 - val_accuracy: 0.9710 - val_loss: 0.0703\n",
      "Epoch 20/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9780 - loss: 0.0608 - val_accuracy: 0.9747 - val_loss: 0.0706\n",
      "Epoch 21/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9784 - loss: 0.0583 - val_accuracy: 0.9733 - val_loss: 0.0685\n",
      "Epoch 22/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9783 - loss: 0.0555 - val_accuracy: 0.9715 - val_loss: 0.0801\n",
      "Epoch 23/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9791 - loss: 0.0565 - val_accuracy: 0.9802 - val_loss: 0.0566\n",
      "Epoch 24/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9826 - loss: 0.0509 - val_accuracy: 0.9770 - val_loss: 0.0588\n",
      "Epoch 25/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9821 - loss: 0.0488 - val_accuracy: 0.9729 - val_loss: 0.0763\n",
      "Epoch 26/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9827 - loss: 0.0517 - val_accuracy: 0.9816 - val_loss: 0.0547\n",
      "Epoch 27/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9839 - loss: 0.0459 - val_accuracy: 0.9807 - val_loss: 0.0589\n",
      "Epoch 28/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9815 - loss: 0.0483 - val_accuracy: 0.9816 - val_loss: 0.0531\n",
      "Epoch 29/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9843 - loss: 0.0441 - val_accuracy: 0.9798 - val_loss: 0.0587\n",
      "Epoch 30/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.0446 - val_accuracy: 0.9821 - val_loss: 0.0605\n",
      "Epoch 31/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9846 - loss: 0.0429 - val_accuracy: 0.9811 - val_loss: 0.0544\n",
      "Epoch 32/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9836 - loss: 0.0446 - val_accuracy: 0.9853 - val_loss: 0.0466\n",
      "Epoch 33/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9842 - loss: 0.0427 - val_accuracy: 0.9867 - val_loss: 0.0475\n",
      "Epoch 34/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9852 - loss: 0.0406 - val_accuracy: 0.9825 - val_loss: 0.0502\n",
      "Epoch 35/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9851 - loss: 0.0390 - val_accuracy: 0.9857 - val_loss: 0.0458\n",
      "Epoch 36/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9859 - loss: 0.0400 - val_accuracy: 0.9830 - val_loss: 0.0481\n",
      "Epoch 37/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9870 - loss: 0.0378 - val_accuracy: 0.9839 - val_loss: 0.0536\n",
      "Epoch 38/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9849 - loss: 0.0409 - val_accuracy: 0.9834 - val_loss: 0.0542\n",
      "Epoch 39/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9853 - loss: 0.0387 - val_accuracy: 0.9825 - val_loss: 0.0577\n",
      "Epoch 40/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9860 - loss: 0.0413 - val_accuracy: 0.9807 - val_loss: 0.0651\n",
      "Epoch 41/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0391 - val_accuracy: 0.9834 - val_loss: 0.0478\n",
      "Epoch 42/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9875 - loss: 0.0330 - val_accuracy: 0.9816 - val_loss: 0.0604\n",
      "Epoch 43/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9897 - loss: 0.0343 - val_accuracy: 0.9825 - val_loss: 0.0555\n",
      "Epoch 44/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9865 - loss: 0.0370 - val_accuracy: 0.9871 - val_loss: 0.0474\n",
      "Epoch 45/100\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9882 - loss: 0.0330 - val_accuracy: 0.9821 - val_loss: 0.0526\n"
     ]
    }
   ],
   "source": [
    "# Mesurer le temps et la mémoire pour l'entraînement\n",
    "tracemalloc.start()\n",
    "start_fit_time = time.time()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "fit_time = time.time() - start_fit_time\n",
    "current, peak = tracemalloc.get_traced_memory()  # Mémoire actuelle et maximale utilisée\n",
    "fit_memory_usage = peak / (1024 * 1024)  # Convertir en Mo\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Mesurer le temps et la mémoire pour l'entraînement\n",
    "tracemalloc.start()\n",
    "start_predict_time = time.time()\n",
    "\n",
    "# Prédire sur le jeu de test\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = y_pred_proba.argmax(axis=1)  # Choisir la classe avec la probabilité la plus élevée\n",
    "\n",
    "predict_time = time.time() - start_predict_time\n",
    "current, peak = tracemalloc.get_traced_memory()  # Mémoire actuelle et maximale utilisée\n",
    "predict_memory_usage = peak / (1024 * 1024)  # Convertir en Mo\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on veut connaitre les perfs pour chaque attaque :\n",
    "# évaluer pour chaque label != normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.str_('normal'): 0,\n",
       " np.str_('DoS'): 1,\n",
       " np.str_('MITM'): 2,\n",
       " np.str_('physical fault'): 3,\n",
       " np.str_('scan'): 4}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['label_mapping'] = label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'data': 'PHY', 'model_type': 'CNN 1D', 'attack_type': np.str_('normal'), 'confusion_matrix': array([[1765,    3,   12,    1,    0],\n",
      "       [   3,   61,    0,    0,    0],\n",
      "       [   1,    0,  190,    0,    0],\n",
      "       [  10,    0,    0,  128,    0],\n",
      "       [   1,    0,    0,    0,    0]]), 'TN': np.int64(379), 'FP': np.int64(15), 'FN': np.int64(16), 'TP': np.int64(1765), 'precision': np.float64(0.9915730337078652), 'recall': np.float64(0.9910162829870859), 'f1': np.float64(0.9912945801741084), 'balanced_accuracy': np.float64(0.976472608498619), 'mcc': np.float64(0.9520054974568799), 'tnr': np.float64(0.9619289340101523), 'fpr': np.float64(0.03807106598984772), 'accuracy': np.float64(0.9857471264367816), 'fit_time': 64.58530044555664, 'predict_time': 0.445847749710083, 'fit_memory_usage': 22.097872734069824, 'predict_memory_usage': 5.592456817626953}, 1: {'data': 'PHY', 'model_type': 'CNN 1D', 'attack_type': np.str_('DoS'), 'confusion_matrix': array([[1765,    3,   12,    1,    0],\n",
      "       [   3,   61,    0,    0,    0],\n",
      "       [   1,    0,  190,    0,    0],\n",
      "       [  10,    0,    0,  128,    0],\n",
      "       [   1,    0,    0,    0,    0]]), 'TN': np.int64(2108), 'FP': np.int64(3), 'FN': np.int64(3), 'TP': np.int64(61), 'precision': np.float64(0.953125), 'recall': np.float64(0.953125), 'f1': np.float64(0.953125), 'balanced_accuracy': np.float64(0.9758519362861203), 'mcc': np.float64(0.9517038725722407), 'tnr': np.float64(0.9985788725722407), 'fpr': np.float64(0.0014211274277593558), 'accuracy': np.float64(0.9972413793103448), 'fit_time': 64.58530044555664, 'predict_time': 0.445847749710083, 'fit_memory_usage': 22.097872734069824, 'predict_memory_usage': 5.592456817626953}, 2: {'data': 'PHY', 'model_type': 'CNN 1D', 'attack_type': np.str_('MITM'), 'confusion_matrix': array([[1765,    3,   12,    1,    0],\n",
      "       [   3,   61,    0,    0,    0],\n",
      "       [   1,    0,  190,    0,    0],\n",
      "       [  10,    0,    0,  128,    0],\n",
      "       [   1,    0,    0,    0,    0]]), 'TN': np.int64(1972), 'FP': np.int64(12), 'FN': np.int64(1), 'TP': np.int64(190), 'precision': np.float64(0.9405940594059405), 'recall': np.float64(0.9947643979057592), 'f1': np.float64(0.9669211195928753), 'balanced_accuracy': np.float64(0.9943580054044925), 'mcc': np.float64(0.9640950591296982), 'tnr': np.float64(0.9939516129032258), 'fpr': np.float64(0.006048387096774193), 'accuracy': np.float64(0.9940229885057471), 'fit_time': 64.58530044555664, 'predict_time': 0.445847749710083, 'fit_memory_usage': 22.097872734069824, 'predict_memory_usage': 5.592456817626953}, 3: {'data': 'PHY', 'model_type': 'CNN 1D', 'attack_type': np.str_('physical fault'), 'confusion_matrix': array([[1765,    3,   12,    1,    0],\n",
      "       [   3,   61,    0,    0,    0],\n",
      "       [   1,    0,  190,    0,    0],\n",
      "       [  10,    0,    0,  128,    0],\n",
      "       [   1,    0,    0,    0,    0]]), 'TN': np.int64(2036), 'FP': np.int64(1), 'FN': np.int64(10), 'TP': np.int64(128), 'precision': np.float64(0.9922480620155039), 'recall': np.float64(0.927536231884058), 'f1': np.float64(0.9588014981273408), 'balanced_accuracy': np.float64(0.9635226569336834), 'mcc': np.float64(0.9567277056828606), 'tnr': np.float64(0.9995090819833088), 'fpr': np.float64(0.0004909180166912126), 'accuracy': np.float64(0.9949425287356322), 'fit_time': 64.58530044555664, 'predict_time': 0.445847749710083, 'fit_memory_usage': 22.097872734069824, 'predict_memory_usage': 5.592456817626953}, 4: {'data': 'PHY', 'model_type': 'CNN 1D', 'attack_type': np.str_('scan'), 'confusion_matrix': array([[1765,    3,   12,    1,    0],\n",
      "       [   3,   61,    0,    0,    0],\n",
      "       [   1,    0,  190,    0,    0],\n",
      "       [  10,    0,    0,  128,    0],\n",
      "       [   1,    0,    0,    0,    0]]), 'TN': np.int64(2174), 'FP': np.int64(0), 'FN': np.int64(1), 'TP': np.int64(0), 'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'balanced_accuracy': np.float64(0.5), 'mcc': 0, 'tnr': np.float64(1.0), 'fpr': np.float64(0.0), 'accuracy': np.float64(0.9995402298850574), 'fit_time': 64.58530044555664, 'predict_time': 0.445847749710083, 'fit_memory_usage': 22.097872734069824, 'predict_memory_usage': 5.592456817626953}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zoe/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/zoe/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/zoe/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/zoe/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/zoe/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la confusion matrix pour tout le test\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Dictionnaire pour stocker les résultats de chaque classe\n",
    "class_results = {}\n",
    "\n",
    "for class_label in range(nb_class):\n",
    "    # Extraire la matrice de confusion pour la classe spécifique\n",
    "    TP = conf_matrix[class_label, class_label]\n",
    "    FP = sum(conf_matrix[:, class_label]) - TP\n",
    "    FN = sum(conf_matrix[class_label, :]) - TP\n",
    "    TN = conf_matrix.sum() - (TP + FP + FN)\n",
    "\n",
    "    # Calcul des métriques pour chaque classe\n",
    "    precision = precision_score(y_test, y_pred, average=None)[class_label]\n",
    "    recall_tpr = recall_score(y_test, y_pred, average=None)[class_label]\n",
    "    tnr = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    fpr = FP / (FP + TN) if (FP + TN) != 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)[class_label]\n",
    "    balanced_acc = (recall_tpr + tnr) / 2\n",
    "    mcc = (TP * TN - FP * FN) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) if (TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) != 0 else 0\n",
    "\n",
    "\n",
    "    # Stocker les résultats dans le dictionnaire\n",
    "    class_results[class_label] = {\n",
    "        'data': 'PHY',\n",
    "        'model_type': 'CNN 1D',\n",
    "        'attack_type': reverse_label_mapping[class_label],\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'TN': TN,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'TP': TP,\n",
    "        'precision': precision,\n",
    "        'recall': recall_tpr,\n",
    "        'f1': f1,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'mcc': mcc,\n",
    "        'tnr': tnr,\n",
    "        'fpr': fpr,\n",
    "        'accuracy': accuracy,\n",
    "        'fit_time': fit_time,\n",
    "        'predict_time': predict_time,\n",
    "        'fit_memory_usage': fit_memory_usage,\n",
    "        'predict_memory_usage': predict_memory_usage\n",
    "    }\n",
    "\n",
    "# Afficher ou enregistrer les résultats pour chaque classe\n",
    "print(class_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99157303 0.953125   0.94059406 0.99224806 0.        ]\n",
      "Résultats pour la classe 0: 0.9915730337078652\n",
      "avec calcul : 0.9915730337078652\n",
      "Résultats pour la classe 1: 0.953125\n",
      "avec calcul : 0.953125\n",
      "Résultats pour la classe 2: 0.9405940594059405\n",
      "avec calcul : 0.9405940594059405\n",
      "Résultats pour la classe 3: 0.9922480620155039\n",
      "avec calcul : 0.9922480620155039\n",
      "Résultats pour la classe 4: 0.0\n",
      "avec calcul : nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zoe/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_45038/3645313826.py:5: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(f\"avec calcul : {class_results[i]['TP'] / (class_results[i]['TP'] + class_results[i]['FP'])}\")\n"
     ]
    }
   ],
   "source": [
    "# vérif\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "for i in range(nb_class):\n",
    "    print(f\"Résultats pour la classe {i}: {class_results[i]['precision']}\")\n",
    "    print(f\"avec calcul : {class_results[i]['TP'] / (class_results[i]['TP'] + class_results[i]['FP'])}\")\n",
    "# ok c'est bon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test tpr 0.953125\n",
      "0.953125\n"
     ]
    }
   ],
   "source": [
    "tp = class_results[1]['TP']\n",
    "fn = class_results[1]['FN']\n",
    "print(\"test tpr\", tp / (tp + fn))\n",
    "print(class_results[1]['recall'])\n",
    "# ok c'est bon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder les données \n",
    "\n",
    "for i in range(1, nb_class):\n",
    "    class_results[i]['model'] = f'CNN1D - label - {reverse_label_mapping[i]}'\n",
    "    db[f'PHY_results_cnn1d_{reverse_label_mapping[i]}'] = class_results[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
